# Base Configuration
defaults:
  - base    # Inherits from base.yaml configuration
  - override /model: hybrid  # Use 'override' to replace the model from base.yaml
  - _self_

# Optuna Search Settings
optuna:
  n_trials: 20          # Will try 20 different hyperparameter combinations
  timeout: null         # No time limit for the search
  pruner:
    _target_: optuna.pruners.MedianPruner  # Stops unpromising trials early
    n_startup_trials: 5                     # Number of trials before pruning starts
    n_warmup_steps: 5                       # Steps before pruning can happen
    interval_steps: 1                       # How often to check for pruning
  sampler:
    _target_: optuna.samplers.TPESampler   # Uses Tree-structured Parzen Estimators
    seed: ${seed}                          # For reproducibility

# Initial Values (will be tuned by Optuna)
optimizer:
  lr: 1e-3           # Starting learning rate
module:
  in_features: 528  # freq * channels = (n_fft // 2 + 1) * 16
  mlp_features: [384]
  block_channels: [24, 24, 24, 24] #(cnn size/number of layers)
  kernel_width: 16  # Total temporal receptive field of 125 samples given 4 layers
  hidden_size: 256 # (rnn size) kevin's been using 128 i think - lansh994
  num_layers: 4 #(rnn
monitor_mode: "min"    # Whether to minimize or maximize the metric


# Training Settings for Search
trainer:
  max_epochs: 40       # Reduced epochs for faster search
train: True           # Enable training during search

# Search Space Documentation
hyperparameter_space:
  lr: [1e-5, 1e-2]                # Log-scale search between these values
  batch_size: [16, 32, 64, 128]   # Will try these specific values
  dropout: [0.0, 0.5]             # Uniform search between these values
  kernel_size: [3, 5, 7, 9]       # Will try these specific kernel sizes
  warmup_steps: [100, 2000]       # Integer search between these values