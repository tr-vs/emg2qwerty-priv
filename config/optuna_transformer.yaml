# Base Configuration
defaults:
  - base    # Inherits from base.yaml configuration
  - override /model: transformer  # Use 'override' to replace the model from base.yaml
  - _self_

# Model Configuration is now loaded from config/model/lstm.yaml

# Optuna Search Settings
optuna:
  n_trials: 20          # Will try 20 different hyperparameter combinations
  timeout: null         # No time limit for the search
  pruner:
    _target_: optuna.pruners.MedianPruner  # Stops unpromising trials early
    n_startup_trials: 5                     # Number of trials before pruning starts
    n_warmup_steps: 5                       # Steps before pruning can happen
    interval_steps: 1                       # How often to check for pruning
  sampler:
    _target_: optuna.samplers.TPESampler   # Uses Tree-structured Parzen Estimators
    seed: ${seed}                          # For reproducibility

# Initial Values (will be tuned by Optuna)
optimizer:
  lr: 0.001            # Starting learning rate
module:
  in_features: 528  # freq * channels = (n_fft // 2 + 1) * 16
  mlp_features: [384]
  block_channels: [24, 24, 24, 24]
  kernel_width: 32  # Total temporal receptive field of 125 samples given 4 layers
  d_model: 512 #dimension of the transformer thang
  nhead: 8 #number of "attention heads"
  num_layers: 2
  dim_feedforward: 2048
  dropout: 0.3
# Training Settings for Search
trainer:
  max_epochs: 25       # Reduced epochs for faster search
train: True           # Enable training during search

# Search Space Documentation
hyperparameter_space:
  lr: [1e-5, 1e-3]                    # Learning rate (log scale)
  batch_size: [16, 32, 64, 128]       # Batch sizes
  hidden_size: [128, 256, 512]        # RNN hidden sizes
  num_layers: [2, 4]                  # RNN number of layers (only 2 or 4)